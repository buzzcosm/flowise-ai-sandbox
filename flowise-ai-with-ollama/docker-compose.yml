services:

  # ##################################################################
  # Flowise: LLMs and Chatbots
  # Flowise is a low-code platform for building LLMs and chatbots.
  # https://github.com/FlowiseAI/Flowise
  # ##################################################################
  flowise:
    image: flowiseai/flowise
    container_name: flowise
    ports:
      - '3000:3000'
    volumes:
      - ./volumes/flowise_data:/root/.flowise
    environment:
      - PORT=3000
      - DATABASE_PATH=/root/.flowise/flowise.db
    restart: unless-stopped
    depends_on:
      - ollama
      - qdrant
      - embedding

  # ##################################################################
  # Qdrant: Vector Database
  # Qdrant is a high-performance vector search engine
  # that is designed to handle large-scale vector data.
  # https://qdrant.tech/
  # ##################################################################
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - '6333:6333'
      - '6334:6334'
    volumes:
      - ./volumes/qdrant_storage:/qdrant/storage
    environment:
      QDRANT__SERVICE__API_KEY: "mysecretapikey"
    restart: unless-stopped

  # ##################################################################
  # Ollama: Local LLMs
  # Ollama is a platform for running large language models (LLMs).
  # https://github.com/ollama/ollama
  # ##################################################################
  ollama:
    build:
      context: ./ollama
    container_name: ollama
    ports:
      - '11434:11434'
    volumes:
      - ./volumes/ollama_data:/root/.ollama
    restart: unless-stopped

  # ##################################################################
  # LocalAI: Local LLMs
  # LocalAI is a local LLM server that can run various models.
  # https://localai.io/
  # https://hub.docker.com/r/localai/localai
  # ##################################################################
  embedding:
    image: localai/localai:latest
    platform: linux/amd64
    container_name: embedding
    ports:
      - "8080:8080"
    volumes:
      - ./volumes/models:/models
    environment:
      - DEBUG=true
      - MODELS_PATH=/models
      - LOCALAI_MODEL=all-MiniLM-L6-v2
      - BACKEND=sentence-transformers
    restart: unless-stopped

